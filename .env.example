# ============================================================================
# EXISTING CONFIGURATION (Keep as is)
# ============================================================================
VITE_SUPABASE_URL=your_supabase_url
VITE_SUPABASE_ANON_KEY=your_supabase_anon_key
DATABASE_URL=your_database_url

# ============================================================================
# AI MAINTENANCE AGENT CONFIGURATION
# ============================================================================

# AI Provider Selection
# Options: 'openai', 'perplexity', 'ollama'
AI_PROVIDER=perplexity

# ============================================================================
# OPTION 1: PERPLEXITY (RECOMMENDED FOR YOUR CASE)
# ============================================================================
# Why Perplexity?
# - Web search capability (finds manufacturer docs online)
# - Multi-language support (Bulgarian, English, Italian, German)
# - Can search recent forum posts and YouTube videos
# - Citations included automatically
# - Cost: ~$0.02-0.05 per request (similar to OpenAI)

# Get your API key from: https://www.perplexity.ai/settings/api
PERPLEXITY_API_KEY=pplx-your-key-here

# Perplexity Models (choose one):
# - llama-3.1-sonar-large-128k-online (RECOMMENDED: best for technical queries with web search)
# - llama-3.1-sonar-small-128k-online (faster, cheaper, still good)
# - llama-3.1-sonar-huge-128k-online (most powerful, more expensive)
PERPLEXITY_MODEL=llama-3.1-sonar-large-128k-online

# ============================================================================
# OPTION 2: OPENAI GPT-4 (Alternative)
# ============================================================================
# Use this if you prefer OpenAI
# Get your API key from: https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-your-key-here
# OPENAI_MODEL=gpt-4

# ============================================================================
# OPTION 3: OLLAMA (Free, Local LLM)
# ============================================================================
# Use this for completely free, on-premise solution
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=mistral

# ============================================================================
# MULTI-LANGUAGE SUPPORT
# ============================================================================
# The AI automatically detects and responds in:
# - Bulgarian (български)
# - English
# - Italian (Italiano)
# - German (Deutsch)

# Default language (if auto-detection fails)
DEFAULT_LANGUAGE=bulgarian

# ============================================================================
# SERVER CONFIGURATION
# ============================================================================
NODE_ENV=production
PORT=3000

# ============================================================================
# OPTIONAL: Advanced AI Configuration
# ============================================================================

# Rate limiting (diagnoses per day per user)
AI_RATE_LIMIT_PER_DAY=100

# Cache duration for similar diagnoses (in seconds)
AI_CACHE_DURATION_SECONDS=1800

# Enable detailed logging
AI_DEBUG_LOGGING=true

# Max tokens per diagnosis
AI_MAX_TOKENS_PER_DIAGNOSIS=2500

# Temperature (0.0-2.0, lower = more consistent, higher = more creative)
AI_TEMPERATURE=0.3
